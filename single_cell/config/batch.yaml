
storage_container_name: 'tasks-container'


pools:
  singlecell${reference}standard_${version}:
    pool_vm_size: 'STANDARD_DS11_V2'
    node_os_publisher: 'Canonical'
    node_os_offer: 'UbuntuServer'
    node_os_sku: '16'
    data_disk_sizes:
    max_tasks_per_node: 2
    auto_scale_formula: |
      numAddMax = 20;
      numDelMax = 20;
      startingNumberOfVMs = 0;
      minNumberofVMs = 0;
      maxNumberofVMs = 1000;
      pendingTaskSamplePercent = $PendingTasks.GetSamplePercent(180 * TimeInterval_Second);
      pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg($PendingTasks.GetSample(180 * TimeInterval_Second));
      cores = $TargetLowPriorityNodes * 2;
      $extraVMs =  (pendingTaskSamples - cores) / 2;
      $extraVMs = min(numAddMax, $extraVMs);
      $extraVMs = max(-numDelMax, $extraVMs);
      targetVMs = ($TargetLowPriorityNodes + $extraVMs);
      $TargetLowPriorityNodes = max(minNumberofVMs,min(maxNumberofVMs, targetVMs));
    # Create VM
    # ---------
    # Install pip, singularity
    # Download the single cell pipeline data to the batch shared directory
    # including the reference data and the singularity container
    create_vm_commands: |
      if [ `sudo blockdev --getsize64 /dev/sdc` -le 40000000000 ]; then sudo mount /dev/sdc /refdata; else sudo mount /dev/sdd /refdata; fi
      if [ `sudo blockdev --getsize64 /dev/sdd` -le 900000000000 ]; then sudo mount /dev/sdd /datadrive; else sudo mount /dev/sdc /datadrive; fi
      sudo chmod -R 777 /datadrive /refdata
      export PATH=/usr/local/miniconda2/bin/:$PATH
      export GIT_SSL_NO_VERIFY=1
      git clone -b 0.1.3 https://dgrewal@svn.bcgsc.ca/bitbucket/scm/sc/single_cell_pipeline.git; cd single_cell_pipeline; python setup.py install; cd ../
      git clone -b azure_production https://dgrewal@bitbucket.org/dranew/pypeliner.git; cd pypeliner; python setup.py install; cd ../
    start_resources:
      startup.sh: /startup-resources/startup.sh

  singlecell${reference}highmem_${version}:
    pool_vm_size: 'STANDARD_DS11_V2'
    node_os_publisher: 'Canonical'
    node_os_offer: 'UbuntuServer'
    node_os_sku: '16'
    data_disk_sizes:
    max_tasks_per_node: 1
    auto_scale_formula: |
      numAddMax = 20;
      numDelMax = 20;
      startingNumberOfVMs = 0;
      minNumberofVMs = 0;
      maxNumberofVMs = 1000;
      pendingTaskSamplePercent = $PendingTasks.GetSamplePercent(180 * TimeInterval_Second);
      pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg($PendingTasks.GetSample(180 * TimeInterval_Second));
      cores = $TargetLowPriorityNodes ;
      $extraVMs =  pendingTaskSamples - cores;
      $extraVMs = min(numAddMax, $extraVMs);
      $extraVMs = max(-numDelMax, $extraVMs);
      targetVMs = ($TargetLowPriorityNodes + $extraVMs);
      $TargetLowPriorityNodes = max(minNumberofVMs,min(maxNumberofVMs, targetVMs));
    create_vm_commands: |
      if [ `sudo blockdev --getsize64 /dev/sdc` -le 40000000000 ]; then sudo mount /dev/sdc /refdata; else sudo mount /dev/sdd /refdata; fi
      if [ `sudo blockdev --getsize64 /dev/sdd` -le 900000000000 ]; then sudo mount /dev/sdd /datadrive; else sudo mount /dev/sdc /datadrive; fi
      sudo chmod -R 777 /datadrive /refdata
      export PATH=/usr/local/miniconda2/bin/:$PATH
      export GIT_SSL_NO_VERIFY=1
      git clone -b 0.1.3 https://dgrewal@svn.bcgsc.ca/bitbucket/scm/sc/single_cell_pipeline.git; cd single_cell_pipeline; python setup.py install; cd ../
      git clone -b azure_production https://dgrewal@bitbucket.org/dranew/pypeliner.git; cd pypeliner; python setup.py install; cd ../
    start_resources:
      startup.sh: /startup-resources/startup.sh

  singlecell${reference}multicore_${version}:
    pool_vm_size: 'STANDARD_DS13_V2'
    node_os_publisher: 'Canonical'
    node_os_offer: 'UbuntuServer'
    node_os_sku: '16'
    data_disk_sizes:
    max_tasks_per_node: 1
    auto_scale_formula: |
      numAddMax = 20;
      numDelMax = 20;
      startingNumberOfVMs = 0;
      minNumberofVMs = 0;
      maxNumberofVMs = 1000;
      pendingTaskSamplePercent = $PendingTasks.GetSamplePercent(180 * TimeInterval_Second);
      pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg($PendingTasks.GetSample(180 * TimeInterval_Second));
      cores = $TargetLowPriorityNodes ;
      $extraVMs =  pendingTaskSamples - cores;
      $extraVMs = min(numAddMax, $extraVMs);
      $extraVMs = max(-numDelMax, $extraVMs);
      targetVMs = ($TargetLowPriorityNodes + $extraVMs);
      $TargetLowPriorityNodes = max(minNumberofVMs,min(maxNumberofVMs, targetVMs));
    create_vm_commands: |
      if [ `sudo blockdev --getsize64 /dev/sdc` -le 40000000000 ]; then sudo mount /dev/sdc /refdata; else sudo mount /dev/sdd /refdata; fi
      if [ `sudo blockdev --getsize64 /dev/sdd` -le 900000000000 ]; then sudo mount /dev/sdd /datadrive; else sudo mount /dev/sdc /datadrive; fi
      sudo chmod -R 777 /datadrive /refdata
      export PATH=/usr/local/miniconda2/bin/:$PATH
      export GIT_SSL_NO_VERIFY=1
      git clone -b 0.1.3 https://dgrewal@svn.bcgsc.ca/bitbucket/scm/sc/single_cell_pipeline.git; cd single_cell_pipeline; python setup.py install; cd ../
      git clone -b azure_production https://dgrewal@bitbucket.org/dranew/pypeliner.git; cd pypeliner; python setup.py install; cd ../
    start_resources:
      startup.sh: /startup-resources/startup.sh


# Compute Task Start Commmands
# ----------------------------
# Development specific command
# Make a development directory including site-packages
# Pull the latest code from the pypeliner and single cell repo
# and make sure they are used by the singularity container
compute_start_commands: |
  clean_up () {
    echo "clean_up task executed"
    find $AZ_BATCH_TASK_WORKING_DIR/ -xtype l -delete
    exit 0
  }
  trap clean_up EXIT
  export PATH=/usr/local/miniconda2/bin/:$PATH
  mkdir -p /datadrive/$AZ_BATCH_TASK_WORKING_DIR/
  cd /datadrive/$AZ_BATCH_TASK_WORKING_DIR/

# Compute Task Run Commmand
# ----------------------------
# Run the singularity container, mapping the development directory
# to the development directory of the container, the singlecellpipeline
# directory to the ref data directory, and the working directory to
# the /datadrive directory for common paths between head vm and compute
# vm.  Run the delegator command in the container.
compute_run_command: >
  pypeliner_delegate $AZ_BATCH_TASK_WORKING_DIR/{input_filename} $AZ_BATCH_TASK_WORKING_DIR/{output_filename}
# Compute Task Finish Commmand
# ----------------------------
# Post command workaround because azure batch fails to upload outputfiles
# if there are broken symlinks in the working directory.
compute_finish_commands: |
  find /datadrive/$AZ_BATCH_TASK_WORKING_DIR/ -xtype l -delete
  find /datadrive/$AZ_BATCH_TASK_WORKING_DIR/ -type f -delete

no_delete_pool: True
no_delete_job: False
